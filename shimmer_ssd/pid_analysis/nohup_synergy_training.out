nohup: ignoring input
2025-08-03 16:48:08,140 - INFO - Configuration validation passed
2025-08-03 16:48:08,141 - INFO - Initialized SynergyTrainer on device: cuda
2025-08-03 16:48:08,141 - INFO - Setting up model...
2025-08-03 16:48:08,436 - INFO - Visual domain: bypassing domain module, using 12D VAE latents directly
2025-08-03 16:48:08,438 - INFO - Attribute domain: bypassing domain module, using 12D input (11D + 1D padding) for cycle compatibility
2025-08-03 16:48:08,438 - INFO - Expanding attr decoder output: 11 -> 12 (+1 synergy)
2025-08-03 16:48:08,439 - INFO - Frozen domain module: v
2025-08-03 16:48:08,440 - INFO - Frozen domain module: attr
2025-08-03 16:48:08,455 - INFO - Model setup complete. Parameters: 3,824
2025-08-03 16:48:08,455 - INFO - Domain v: latent_dim=12
2025-08-03 16:48:08,455 - INFO - Domain attr: latent_dim=12
2025-08-03 16:48:08,455 - INFO - Setting up data loaders...
2025-08-03 16:48:08,455 - INFO - Validated synergy config for domains: ['v', 'attr']
2025-08-03 16:48:08,514 - INFO - Loaded original attributes: shape (500000, 8)
2025-08-03 16:48:08,514 - INFO - Loading VAE latents from: /home/janerik/shimmer-ssd/simple_shapes_dataset/saved_latents/train/calmip-822888_epoch=282-step=1105680_future.npy
2025-08-03 16:48:10,013 - INFO - VAE latents (1000000) have more samples than XOR data (500000). Using first 500000 latents.
2025-08-03 16:48:10,019 - INFO - ‚úÖ Loaded 500000 VAE latents: shape (500000, 2, 12), range [-4.984, 4.413]
2025-08-03 16:48:10,051 - INFO - Extracted VAE mean vectors: torch.Size([500000, 12])
2025-08-03 16:48:10,175 - INFO - Prepared input/target tensors for all domains
2025-08-03 16:48:10,175 - INFO -   v: input torch.Size([500000, 12]), target torch.Size([500000, 12])
2025-08-03 16:48:10,176 - INFO -   attr: input torch.Size([500000, 11]), target torch.Size([500000, 12])
2025-08-03 16:48:10,176 - INFO - Loaded 500000 samples for train split
2025-08-03 16:48:10,176 - INFO - Initialized SynergyDataset for train split with 500000 samples
2025-08-03 16:48:10,177 - INFO - Synergy domains: ['domains', 'feature_indices']
2025-08-03 16:48:10,177 - INFO - Created train dataloader with 500000 samples
2025-08-03 16:48:10,177 - INFO - Validated synergy config for domains: ['v', 'attr']
2025-08-03 16:48:10,184 - INFO - Loaded original attributes: shape (50000, 8)
2025-08-03 16:48:10,184 - INFO - Loading VAE latents from: /home/janerik/shimmer-ssd/simple_shapes_dataset/saved_latents/val/calmip-822888_epoch=282-step=1105680_future.npy
2025-08-03 16:48:10,186 - INFO - ‚úÖ Loaded 50000 VAE latents: shape (50000, 2, 12), range [-4.899, 4.131]
2025-08-03 16:48:10,196 - INFO - Extracted VAE mean vectors: torch.Size([50000, 12])
2025-08-03 16:48:10,281 - INFO - Prepared input/target tensors for all domains
2025-08-03 16:48:10,282 - INFO -   v: input torch.Size([50000, 12]), target torch.Size([50000, 12])
2025-08-03 16:48:10,282 - INFO -   attr: input torch.Size([50000, 11]), target torch.Size([50000, 12])
2025-08-03 16:48:10,282 - INFO - Loaded 50000 samples for val split
2025-08-03 16:48:10,282 - INFO - Initialized SynergyDataset for val split with 50000 samples
2025-08-03 16:48:10,282 - INFO - Synergy domains: ['domains', 'feature_indices']
2025-08-03 16:48:10,282 - INFO - Created val dataloader with 50000 samples
2025-08-03 16:48:10,283 - INFO - Validated synergy config for domains: ['v', 'attr']
2025-08-03 16:48:10,287 - INFO - Loaded original attributes: shape (50000, 8)
2025-08-03 16:48:10,287 - INFO - Loading VAE latents from: /home/janerik/shimmer-ssd/simple_shapes_dataset/saved_latents/test/calmip-822888_epoch=282-step=1105680_future.npy
2025-08-03 16:48:10,291 - INFO - ‚úÖ Loaded 50000 VAE latents: shape (50000, 2, 12), range [-5.132, 4.110]
2025-08-03 16:48:10,304 - INFO - Extracted VAE mean vectors: torch.Size([50000, 12])
2025-08-03 16:48:10,388 - INFO - Prepared input/target tensors for all domains
2025-08-03 16:48:10,388 - INFO -   v: input torch.Size([50000, 12]), target torch.Size([50000, 12])
2025-08-03 16:48:10,388 - INFO -   attr: input torch.Size([50000, 11]), target torch.Size([50000, 12])
2025-08-03 16:48:10,388 - INFO - Loaded 50000 samples for test split
2025-08-03 16:48:10,388 - INFO - Initialized SynergyDataset for test split with 50000 samples
2025-08-03 16:48:10,388 - INFO - Synergy domains: ['domains', 'feature_indices']
2025-08-03 16:48:10,389 - INFO - Created test dataloader with 50000 samples
2025-08-03 16:48:10,389 - INFO - train: 500000 samples, 15625 batches
2025-08-03 16:48:10,389 - INFO - val: 50000 samples, 1563 batches
2025-08-03 16:48:10,389 - INFO - test: 50000 samples, 1563 batches
2025-08-03 16:48:10,389 - INFO - Running 1 experiments...
2025-08-03 16:48:10,389 - INFO - 
============================================================
2025-08-03 16:48:10,389 - INFO - Experiment 1/1
2025-08-03 16:48:10,389 - INFO - ============================================================
2025-08-03 16:48:10,390 - INFO - Starting experiment: all_losses
2025-08-03 16:48:10,390 - INFO - Loss weights: {'fusion': 0.3, 'demi_cycle': 0.2, 'cycle': 0.2, 'translation': 0.3}
wandb: Currently logged in as: bellingrathjanerik (cerco_neuro_ai) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /home/janerik/GLW_PID/GLW_PID_repo/shimmer_ssd/pid_analysis/wandb/run-20250803_164810-cq7kb1tw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run all_losses
wandb: ‚≠êÔ∏è View project at https://wandb.ai/cerco_neuro_ai/synergy-glw-pid
wandb: üöÄ View run at https://wandb.ai/cerco_neuro_ai/synergy-glw-pid/runs/cq7kb1tw
üîí GPU memory limited to 10GB (25% of 40GB GPU)
Loaded domain module: v
Loaded domain module: attr
Training:   0%|          | 0/50 [00:00<?, ?it/s]Training:   2%|‚ñè         | 1/50 [12:50<10:28:56, 770.14s/it]Training:   4%|‚ñç         | 2/50 [25:34<10:13:18, 766.63s/it]Training:   6%|‚ñå         | 3/50 [38:16<9:58:56, 764.61s/it] Training:   8%|‚ñä         | 4/50 [51:06<9:47:49, 766.73s/it]Training:  10%|‚ñà         | 5/50 [1:03:57<9:36:04, 768.10s/it]Training:  12%|‚ñà‚ñè        | 6/50 [1:16:43<9:22:52, 767.56s/it]Training:  14%|‚ñà‚ñç        | 7/50 [1:29:26<9:09:04, 766.16s/it]Training:  16%|‚ñà‚ñå        | 8/50 [1:42:11<8:56:03, 765.81s/it]Training:  18%|‚ñà‚ñä        | 9/50 [1:54:53<8:42:23, 764.48s/it]Training:  20%|‚ñà‚ñà        | 10/50 [2:07:34<8:28:58, 763.45s/it]Training:  22%|‚ñà‚ñà‚ñè       | 11/50 [2:18:28<7:54:21, 729.79s/it]Training:  24%|‚ñà‚ñà‚ñç       | 12/50 [2:27:10<7:02:14, 666.71s/it]Training:  26%|‚ñà‚ñà‚ñå       | 13/50 [2:35:54<6:24:28, 623.48s/it]Training:  28%|‚ñà‚ñà‚ñä       | 14/50 [2:44:40<5:56:27, 594.11s/it]Training:  30%|‚ñà‚ñà‚ñà       | 15/50 [2:53:25<5:34:19, 573.13s/it]Training:  32%|‚ñà‚ñà‚ñà‚ñè      | 16/50 [3:02:09<5:16:31, 558.57s/it]Training:  34%|‚ñà‚ñà‚ñà‚ñç      | 17/50 [3:10:57<5:02:01, 549.14s/it]Training:  36%|‚ñà‚ñà‚ñà‚ñå      | 18/50 [3:19:43<4:49:15, 542.36s/it]Training:  38%|‚ñà‚ñà‚ñà‚ñä      | 19/50 [3:28:31<4:38:01, 538.10s/it]Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 20/50 [3:37:14<4:26:46, 533.54s/it]Training:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21/50 [3:45:59<4:16:38, 530.97s/it]Training:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 22/50 [3:54:50<4:07:48, 531.01s/it]Training:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23/50 [4:03:39<3:58:36, 530.23s/it]Training:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 24/50 [4:12:26<3:49:23, 529.37s/it]Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 25/50 [4:21:10<3:39:55, 527.84s/it]Training:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26/50 [4:29:55<3:30:44, 526.85s/it]Training:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 27/50 [4:38:57<3:23:43, 531.46s/it]Training:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28/50 [4:47:41<3:14:03, 529.27s/it]Training:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 29/50 [4:56:33<3:05:28, 529.93s/it]Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 30/50 [5:05:22<2:56:33, 529.65s/it]Training:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 31/50 [5:14:08<2:47:23, 528.59s/it]Training:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 32/50 [5:22:56<2:38:30, 528.35s/it]Training:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 33/50 [5:31:48<2:30:03, 529.63s/it]Training:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 34/50 [5:40:37<2:21:06, 529.18s/it]Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 35/50 [5:49:21<2:11:54, 527.65s/it]Training:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 36/50 [5:58:14<2:03:29, 529.28s/it]Training:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 37/50 [6:07:03<1:54:39, 529.19s/it]Epoch 1/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 1/50, Train Loss: 118.425887, Val Loss: 20.109834, Time: 0:12:50.137634
Epoch 2/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 2/50, Train Loss: 17.593896, Val Loss: 15.956217, Time: 0:12:44.179367
Epoch 3/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 3/50, Train Loss: 14.895814, Val Loss: 14.416041, Time: 0:12:42.194396
Epoch 4/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 4/50, Train Loss: 13.539058, Val Loss: 13.136203, Time: 0:12:49.993596
Epoch 5/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 5/50, Train Loss: 12.655981, Val Loss: 11.825933, Time: 0:12:50.506059
Epoch 6/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 6/50, Train Loss: 12.047829, Val Loss: 11.631167, Time: 0:12:46.515823
Epoch 7/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 7/50, Train Loss: 11.521065, Val Loss: 11.569339, Time: 0:12:43.264551
Epoch 8/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 8/50, Train Loss: 11.179490, Val Loss: 11.190913, Time: 0:12:45.064712
Epoch 9/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 9/50, Train Loss: 10.920272, Val Loss: 10.450914, Time: 0:12:41.553417
Epoch 10/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 10/50, Train Loss: 10.726681, Val Loss: 10.915868, Time: 0:12:41.137803
Epoch 11/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 11/50, Train Loss: 10.563828, Val Loss: 10.427212, Time: 0:10:53.468185
Epoch 12/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 12/50, Train Loss: 10.406078, Val Loss: 10.113388, Time: 0:08:42.413903
Epoch 13/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 13/50, Train Loss: 10.214999, Val Loss: 9.739556, Time: 0:08:44.006680
Epoch 14/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 14/50, Train Loss: 9.927446, Val Loss: 9.330179, Time: 0:08:46.248768
Epoch 15/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 15/50, Train Loss: 9.601547, Val Loss: 9.303361, Time: 0:08:44.502874
Epoch 16/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 16/50, Train Loss: 9.233369, Val Loss: 8.685793, Time: 0:08:44.742699
Epoch 17/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 17/50, Train Loss: 8.845636, Val Loss: 8.672435, Time: 0:08:47.217788
Epoch 18/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 18/50, Train Loss: 8.644316, Val Loss: 9.031658, Time: 0:08:46.585109
Epoch 19/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 19/50, Train Loss: 8.496469, Val Loss: 8.382010, Time: 0:08:48.165945
Epoch 20/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 20/50, Train Loss: 8.312769, Val Loss: 8.032909, Time: 0:08:42.906302
Epoch 21/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 21/50, Train Loss: 8.210016, Val Loss: 8.229331, Time: 0:08:44.977504
Epoch 22/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 22/50, Train Loss: 8.144328, Val Loss: 8.137226, Time: 0:08:51.099661
Epoch 23/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 23/50, Train Loss: 8.092365, Val Loss: 7.672281, Time: 0:08:48.422053
Epoch 24/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 24/50, Train Loss: 8.031663, Val Loss: 7.898192, Time: 0:08:47.342316
Epoch 25/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 25/50, Train Loss: 7.963363, Val Loss: 7.911549, Time: 0:08:44.268774
Epoch 26/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 26/50, Train Loss: 7.914624, Val Loss: 7.580654, Time: 0:08:44.535717
Epoch 27/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 27/50, Train Loss: 7.870668, Val Loss: 8.175251, Time: 0:09:02.204446
Epoch 28/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 28/50, Train Loss: 7.839525, Val Loss: 7.710233, Time: 0:08:44.167835
Epoch 29/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 29/50, Train Loss: 7.778427, Val Loss: 7.452726, Time: 0:08:51.484579
Epoch 30/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 30/50, Train Loss: 7.734749, Val Loss: 8.256354, Time: 0:08:48.984071
Epoch 31/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 31/50, Train Loss: 7.694825, Val Loss: 7.541401, Time: 0:08:46.114208
Epoch 32/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 32/50, Train Loss: 7.653007, Val Loss: 7.543384, Time: 0:08:47.773756
Epoch 33/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 33/50, Train Loss: 7.595804, Val Loss: 7.418884, Time: 0:08:52.610448
Epoch 34/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 34/50, Train Loss: 7.542779, Val Loss: 7.304063, Time: 0:08:48.132229
Epoch 35/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 35/50, Train Loss: 7.497304, Val Loss: 7.357609, Time: 0:08:44.074182
Epoch 36/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 36/50, Train Loss: 7.487813, Val Loss: 7.109038, Time: 0:08:53.071866
Epoch 37/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 37/50, Train Loss: 7.441177, Val Loss: 7.324407, Time: 0:08:48.987525
Epoch 38/50
Training:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 38/50 [6:15:58<1:46:12, 531.04s/it]Training:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 39/50 [6:24:47<1:37:13, 530.31s/it]Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 40/50 [6:33:38<1:28:25, 530.51s/it]Training:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 41/50 [6:42:20<1:19:12, 528.01s/it]Training:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 42/50 [6:51:08<1:10:23, 527.94s/it]Training:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 43/50 [6:59:55<1:01:33, 527.70s/it]Training:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 44/50 [7:08:40<52:41, 526.94s/it]  Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 45/50 [7:17:31<44:01, 528.33s/it]Training:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 46/50 [7:26:16<35:08, 527.23s/it]Training:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 47/50 [7:34:57<26:16, 525.48s/it]Training:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 48/50 [7:43:47<17:33, 526.80s/it]Training:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 49/50 [7:52:40<08:48, 528.54s/it]Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [8:01:26<00:00, 527.74s/it]Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [8:01:26<00:00, 577.73s/it]
wandb: uploading output.log; uploading wandb-summary.json
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                                        epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                                   epoch_time ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train_cycle_attr_via_v_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                             train_cycle_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train_cycle_v_via_attr_loss ‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   train_demi_cycle_attr_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                        train_demi_cycle_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      train_demi_cycle_v_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                       train_fusion_attr_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train_fusion_attr_non_synergy_loss ‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train_fusion_attr_synergy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñà‚ñà
wandb:                            train_fusion_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                          train_fusion_v_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              train_fusion_v_non_synergy_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                   train_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                  train_num_translation_pairs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                             train_total_loss ‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train_translation_attr_to_v_masked_synergy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_translation_attr_to_v_non_synergy_loss ‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:   train_translation_attr_to_v_paired_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    train_translation_attr_to_v_total_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                       train_translation_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train_translation_v_to_attr_masked_synergy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: train_translation_v_to_attr_non_synergy_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train_translation_v_to_attr_paired_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    train_translation_v_to_attr_total_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    train_weighted_cycle_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:               train_weighted_demi_cycle_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                   train_weighted_fusion_loss ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:              train_weighted_translation_loss ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    val_cycle_attr_via_v_loss ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÜ
wandb:                               val_cycle_loss ‚ñá‚ñá‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÜ
wandb:                    val_cycle_v_via_attr_loss ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÉ
wandb:                     val_demi_cycle_attr_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ
wandb:                          val_demi_cycle_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ
wandb:                        val_demi_cycle_v_loss ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                         val_fusion_attr_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:             val_fusion_attr_non_synergy_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                 val_fusion_attr_synergy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà
wandb:                              val_fusion_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ
wandb:                            val_fusion_v_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                val_fusion_v_non_synergy_loss ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                                     val_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                    val_num_translation_pairs ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                               val_total_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                              val_train_ratio ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà
wandb:     val_translation_attr_to_v_masked_synergy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_translation_attr_to_v_non_synergy_loss ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:     val_translation_attr_to_v_paired_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      val_translation_attr_to_v_total_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                         val_translation_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     val_translation_v_to_attr_masked_synergy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   val_translation_v_to_attr_non_synergy_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:     val_translation_v_to_attr_paired_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:      val_translation_v_to_attr_total_samples ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                      val_weighted_cycle_loss ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñÜ
wandb:                 val_weighted_demi_cycle_loss ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÑ
wandb:                     val_weighted_fusion_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÉ
wandb:                val_weighted_translation_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                                best_val_loss 7.06259
wandb:                                        epoch 50
wandb:                                   epoch_time 525.85401
wandb:                             epochs_completed 50
wandb:                             final_checkpoint experiments/synergy/...
wandb:                             final_train_loss 7.26261
wandb:                               final_val_loss 8.02107
wandb:                                learning_rate 0.001
wandb:                          total_training_time 28886.35179
wandb:                  train_cycle_attr_via_v_loss 4.85102
wandb:                             train_cycle_loss 2.81776
wandb:                  train_cycle_v_via_attr_loss 0.7845
wandb:                   train_demi_cycle_attr_loss 3.4306
wandb:                        train_demi_cycle_loss 2.0587
wandb:                      train_demi_cycle_v_loss 0.6868
wandb:                       train_fusion_attr_loss 3.8533
wandb:           train_fusion_attr_non_synergy_loss 3.89861
wandb:               train_fusion_attr_synergy_loss 3.35487
wandb:                            train_fusion_loss 2.26225
wandb:                          train_fusion_v_loss 0.67121
wandb:              train_fusion_v_non_synergy_loss 0.67121
wandb:                                   train_loss 7.26261
wandb:                  train_num_translation_pairs 2
wandb:                             train_total_loss 7.26261
wandb:   train_translation_attr_to_v_masked_synergy 1
wandb: train_translation_attr_to_v_non_synergy_loss 1.19189
wandb:   train_translation_attr_to_v_paired_samples 32
wandb:    train_translation_attr_to_v_total_samples 32
wandb:                       train_translation_loss 18.69547
wandb:   train_translation_v_to_attr_masked_synergy 1
wandb: train_translation_v_to_attr_non_synergy_loss 36.19906
wandb:   train_translation_v_to_attr_paired_samples 32
wandb:    train_translation_v_to_attr_total_samples 32
wandb:                    train_weighted_cycle_loss 0.56355
wandb:               train_weighted_demi_cycle_loss 0.41174
wandb:                   train_weighted_fusion_loss 0.67868
wandb:              train_weighted_translation_loss 5.60864
wandb:                            use_weighted_loss False
wandb:                    val_cycle_attr_via_v_loss 7.85747
wandb:                               val_cycle_loss 4.33281
wandb:                    val_cycle_v_via_attr_loss 0.80815
wandb:                     val_demi_cycle_attr_loss 5.65259
wandb:                          val_demi_cycle_loss 3.17282
wandb:                        val_demi_cycle_v_loss 0.69304
wandb:                         val_fusion_attr_loss 6.47598
wandb:             val_fusion_attr_non_synergy_loss 3.89998
wandb:                 val_fusion_attr_synergy_loss 34.81201
wandb:                              val_fusion_loss 3.57443
wandb:                            val_fusion_v_loss 0.67288
wandb:                val_fusion_v_non_synergy_loss 0.67288
wandb:                                     val_loss 8.02107
wandb:                    val_num_translation_pairs 2
wandb:                               val_total_loss 8.02107
wandb:                              val_train_ratio 1.10443
wandb:     val_translation_attr_to_v_masked_synergy 1
wandb:   val_translation_attr_to_v_non_synergy_loss 1.17962
wandb:     val_translation_attr_to_v_paired_samples 31.98976
wandb:      val_translation_attr_to_v_total_samples 31.98976
wandb:                         val_translation_loss 18.15873
wandb:     val_translation_v_to_attr_masked_synergy 1
wandb:   val_translation_v_to_attr_non_synergy_loss 35.13784
wandb:     val_translation_v_to_attr_paired_samples 31.98976
wandb:      val_translation_v_to_attr_total_samples 31.98976
wandb:                      val_weighted_cycle_loss 0.86656
wandb:                 val_weighted_demi_cycle_loss 0.63456
wandb:                     val_weighted_fusion_loss 1.07233
wandb:                val_weighted_translation_loss 5.44762
wandb: 
wandb: üöÄ View run all_losses at: https://wandb.ai/cerco_neuro_ai/synergy-glw-pid/runs/cq7kb1tw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/cerco_neuro_ai/synergy-glw-pid
wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250803_164810-cq7kb1tw/logs
Traceback (most recent call last):
  File "/home/janerik/GLW_PID/GLW_PID_repo/shimmer_ssd/pid_analysis/losses_and_weights_GLW_training.py", line 996, in evaluate_model
    batch_loss, loss_details = calculate_losses_with_weights(
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/GLW_PID/GLW_PID_repo/shimmer_ssd/pid_analysis/losses_and_weights_GLW_training.py", line 1497, in calculate_losses_with_weights
    fusion_loss, fusion_details = calculate_fusion_loss(
                                  ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/GLW_PID/GLW_PID_repo/shimmer_ssd/pid_analysis/losses_and_weights_GLW_training.py", line 1763, in calculate_fusion_loss
    encoded[domain_name] = model.gw_encoders[domain_name](domain_input)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/.local/lib/python3.11/site-packages/shimmer/modules/gw_module.py", line 205, in forward
    return super().forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/janerik/miniconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x11 and 12x16)
2025-08-04 00:49:40,490 - INFO - Completed experiment: all_losses
2025-08-04 00:49:40,490 - INFO - Final validation loss: 0.000000
2025-08-04 00:49:40,490 - INFO - 
All experiments completed. Results saved to experiments/synergy/experiment_results.json
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 38/50, Train Loss: 7.428155, Val Loss: 7.062591, Time: 0:08:55.352229
Epoch 39/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 39/50, Train Loss: 7.386880, Val Loss: 7.463626, Time: 0:08:48.591572
Epoch 40/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 40/50, Train Loss: 7.369243, Val Loss: 7.550488, Time: 0:08:50.972540
Epoch 41/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 41/50, Train Loss: 7.360357, Val Loss: 7.101058, Time: 0:08:42.170905
Epoch 42/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 42/50, Train Loss: 7.333791, Val Loss: 7.094353, Time: 0:08:47.786169
Epoch 43/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 43/50, Train Loss: 7.279335, Val Loss: 7.762986, Time: 0:08:47.133292
Epoch 44/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 44/50, Train Loss: 7.298336, Val Loss: 7.087817, Time: 0:08:45.153890
Epoch 45/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 45/50, Train Loss: 7.237911, Val Loss: 7.461602, Time: 0:08:51.580115
Epoch 46/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 46/50, Train Loss: 7.246034, Val Loss: 7.827621, Time: 0:08:44.670036
Epoch 47/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 47/50, Train Loss: 7.244516, Val Loss: 7.413660, Time: 0:08:41.398724
Epoch 48/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 48/50, Train Loss: 7.267139, Val Loss: 7.274701, Time: 0:08:49.880750
Epoch 49/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 49/50, Train Loss: 7.184649, Val Loss: 7.166697, Time: 0:08:52.602153
Epoch 50/50
  Can't determine dataset size, progress bar disabled
Running full evaluation
  Can't determine dataset size, progress bar disabled
Epoch 50/50, Train Loss: 7.262611, Val Loss: 8.021075, Time: 0:08:45.854010
Final model saved to experiments/synergy/all_losses/gw_model_attr_use_weights_for_loss_v_v_0.5_attr_0.5_use_weights_for_loss_0.0_ws12_20250804_004938.pt
Total training time: 8:01:26.351792
Running full evaluation
  Can't determine dataset size, progress bar disabled
Error during evaluation: mat1 and mat2 shapes cannot be multiplied (32x11 and 12x16)

======================================================================
EXPERIMENT SUMMARY
======================================================================
all_losses          : Best Val Loss = 0.000000 (epoch 50)

Results saved to: experiments/synergy
